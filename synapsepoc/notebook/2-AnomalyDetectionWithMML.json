{
	"name": "2-AnomalyDetectionWithMML",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "ws1sparkpool1",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 19,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "19",
				"spark.dynamicAllocation.maxExecutors": "19",
				"spark.autotune.trackingId": "74a3352b-3d2a-47d4-a523-0fa4b058626a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/41918b4b-7cb5-4e49-a80a-8ef16287837d/resourceGroups/ML-SalesForecast-Retail/providers/Microsoft.Synapse/workspaces/sfrxvi5p4dlnq3hepocws1/bigDataPools/ws1sparkpool1",
				"name": "ws1sparkpool1",
				"type": "Spark",
				"endpoint": "https://sfrxvi5p4dlnq3hepocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ws1sparkpool1",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 5,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Leverage Power of Azure Synapse Link for Cosmos DB and Spark SQL\n",
					"With Synapse Link, you can now directly connect to your Azure Cosmos DB containers from Azure Synapse Analytics and access the analytical store with no separate connectors. This notebook scenario is to \n",
					"\n",
					"- Ingest streaming data into Azure Cosmos DB collection using Structured Streaming\n",
					"- Ingest Batch data into Azure Cosmos DB collection using Azure Synapse Spark\n",
					"- Format the stream dataframe as per the IoTSignals schema\n",
					"- Write the streaming dataframe to the Azure Cosmos DB collection\n",
					"- Perform Joins and aggregations across Azure Cosmos DB collections using Azure Synapse Link\n",
					"- Perform Anomaly Detection using Azure Synapse Link and Azure Cognitive Services on Synapse Spark (MMLSpark)"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/synapse-cosmosdb.png\" alt=\"Surface Device\" width=\"75%\"/>\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Replace the Cognitive Services API account Keys below before execution.Keys can be found under \"Keys and Endpoints\" Section\n",
					"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/AD_Ws_Varriables.gif\" alt=\"Surface Device\" width=\"75%\"/>\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Cognitive_Services_Key='d2697b0a802b45bea7c8c8969320daa1'"
				],
				"execution_count": 21
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Simulate Streaming Data Generation using Rate Streaming Source\n",
					"The Rate streaming source is used to simplify the solution here and can be replaced with any supported streaming sources such as Azure Event Hubs and Apache Kafka."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"dfStream = (spark\n",
					"                .readStream\n",
					"                .format(\"rate\")\n",
					"                .option(\"rowsPerSecond\", 10)\n",
					"                .load()\n",
					"            )"
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Format the Stream Dataframe as per the IoTSignals Schema"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"import pyspark.sql.functions as F\n",
					"from pyspark.sql.types import StringType\n",
					"import uuid\n",
					"\n",
					"numberOfDevices = 10\n",
					"generate_uuid = F.udf(lambda : str(uuid.uuid4()), StringType())\n",
					"              \n",
					"dfIoTSignals = (dfStream\n",
					"                    .withColumn(\"id\", generate_uuid())\n",
					"                    .withColumn(\"deviceId\", F.concat(F.lit(\"dev-\"), F.expr(\"mod(value, %d)\" % numberOfDevices)))\n",
					"                    .withColumn(\"dateTime\", dfStream[\"timestamp\"].cast(StringType()))\n",
					"                    .withColumn(\"unit\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Revolutions per Minute' ELSE 'MegaWatts' END\"))\n",
					"                    .withColumn(\"unitSymbol\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'RPM' ELSE 'MW' END\"))\n",
					"                    .withColumn(\"measureType\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Rotation Speed' ELSE 'Output' END\"))\n",
					"                    .withColumn(\"measureValue\", F.expr(\"CASE WHEN rand() > 0.95 THEN value * 10 WHEN rand() < 0.05 THEN value div 10 ELSE value END\"))\n",
					"                    .drop(\"timestamp\", \"value\")\n",
					"                )"
				],
				"execution_count": 23
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Stream Writes to the Azure Cosmos DB Collection\n",
					"The \"cosmos.oltp\" is the Spark format that enables connection to the Cosmos DB Transactional store."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Stream IoTSignals into Cosmos DB via Synapse Link\n",
					"streamQuery = (\n",
					"    dfIoTSignals\n",
					"      .writeStream\n",
					"      .format(\"cosmos.oltp\")\n",
					"      .outputMode(\"append\")\n",
					"      .option(\"spark.synapse.linkedService\", \"CosmosDBLink\")\n",
					"      .option(\"spark.cosmos.container\", \"IoTSignals\")\n",
					"      .option(\"spark.cosmos.write.strategy\", \"ItemOverwrite\")  # performs upsert \n",
					"      .option(\"checkpointLocation\", \"/writeCheckpointDir\")\n",
					"      .start()\n",
					")\n",
					"\n",
					"# Let it run for up to 3 minutes, then stop\n",
					"streamQuery.awaitTermination(180)\n",
					"streamQuery.stop()\n",
					""
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the IoTDeviceInfo Dataset from ADLS Gen2 to a Dataframe and Write the Dataframe to the Azure Cosmos DB Collection\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import pandas as pd\n",
					"from pyspark.sql.functions import col\n",
					"\n",
					"# 1) Pull IoTDeviceInfo into pandas\n",
					"url = (\n",
					"    \"https://raw.githubusercontent.com/Azure-Samples/Synapse/\"\n",
					"    \"main/Notebooks/PySpark/Synapse%20Link%20for%20Cosmos%20DB%20samples/\"\n",
					"    \"IoT/IoTData/IoTDeviceInfo.csv\"\n",
					")\n",
					"print(\"▶️ Downloading IoTDeviceInfo.csv…\")\n",
					"pdf = pd.read_csv(url)\n",
					"\n",
					"# 2) Convert to Spark DF and add an 'id' column from 'deviceid'\n",
					"sdf = (\n",
					"    spark\n",
					"      .createDataFrame(pdf)\n",
					"      .withColumn(\"id\", col(\"deviceid\").cast(\"string\"))\n",
					")\n",
					"\n",
					"# 3) Write into Cosmos with upsert strategy\n",
					"print(\"✉️ Writing into Cosmos container 'IoTDeviceInfo'…\")\n",
					"(\n",
					"    sdf.write\n",
					"       .format(\"cosmos.oltp\")\n",
					"       .option(\"spark.synapse.linkedService\", \"CosmosDBLink\")\n",
					"       .option(\"spark.cosmos.container\", \"IoTDeviceInfo\")\n",
					"       .option(\"spark.cosmos.write.strategy\", \"ItemOverwrite\")  # upsert\n",
					"       .mode(\"append\")\n",
					"       .save()\n",
					")\n",
					"\n",
					"print(\"✅ IoTDeviceInfo ingested successfully.\")\n",
					"\n",
					""
				],
				"execution_count": 25
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Create Spark Tables Pointing to the Azure Cosmos DB Analytical Store Collections using Azure Synapse Link\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"create database if not exists CosmosDBIoTDemoDB;\n",
					"\n",
					"create table if not exists CosmosDBIoTDemoDB.IoTSignals\n",
					"using cosmos.olap\n",
					"options(spark.synapse.linkedService 'CosmosDBLink',\n",
					"        spark.cosmos.container 'IoTSignals');\n",
					"\n",
					"create table if not exists CosmosDBIoTDemoDB.IoTDeviceInfo\n",
					"using cosmos.olap\n",
					"options(spark.synapse.linkedService 'CosmosDBLink',\n",
					"        spark.cosmos.container 'IoTDeviceInfo')\n",
					""
				],
				"execution_count": 26
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Perform Joins Across collections, Apply Filters and Aggregations using Spark SQL"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df_RPM_details = spark.sql(\"select a.deviceid \\\n",
					"                                 , b.devicetype \\\n",
					"                                 , cast(b.location as string) as location\\\n",
					"                                 , cast(b.latitude as float) as latitude\\\n",
					"                                 , cast(b.longitude as float) as  longitude\\\n",
					"                                 , a.measuretype \\\n",
					"                                 , a.unitSymbol \\\n",
					"                                 , cast(sum(measureValue) as float) as measureValueSum \\\n",
					"                                 , count(*) as count \\\n",
					"                            from CosmosDBIoTDemoDB.IoTSignals a \\\n",
					"                            left join CosmosDBIoTDemoDB.IoTDeviceInfo b \\\n",
					"                            on a.deviceid = b.deviceid \\\n",
					"                            where a.unitSymbol = 'RPM' \\\n",
					"                            group by a.deviceid, b.devicetype, b.location, b.latitude, b.longitude, a.measuretype, a.unitSymbol\")\n",
					"\n",
					"display(df_RPM_details)"
				],
				"execution_count": 27
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Visualizations using Plotly and DisplayHTML()\n",
					"Heatmap of IoT signals across diffrent locations\n",
					"\n",
					"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/HeatMaPlot.png\" alt=\"Chart\" width=\"75%\"/>\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from plotly.offline import plot\n",
					"import plotly.express as px\n",
					"\n",
					"df_RPM_details_pd = df_RPM_details.toPandas()\n",
					"fig = px.scatter_mapbox(df_RPM_details_pd, \n",
					"                        lat='latitude', \n",
					"                        lon='longitude', \n",
					"                        size = 'measureValueSum',\n",
					"                        color = 'measureValueSum',\n",
					"                        hover_name = 'location',\n",
					"                        hover_data = ['measureValueSum','location'],\n",
					"                        size_max = 30,\n",
					"                        color_continuous_scale = px.colors.carto.Temps,\n",
					"                        zoom=3,\n",
					"                        height=600,\n",
					"                        width =900)\n",
					"\n",
					"fig.update_layout(mapbox_style='open-street-map')\n",
					"fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
					"\n",
					"p = plot(fig,output_type='div')\n",
					"displayHTML(p)"
				],
				"execution_count": 28
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load the Data in Cosmos DB Analytical Store Collection into a Dataframe using Synapse Link"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"df_IoTSignals = spark.read\\\n",
					"                    .format(\"cosmos.olap\")\\\n",
					"                    .option(\"spark.synapse.linkedService\", \"CosmosDBLink\")\\\n",
					"                    .option(\"spark.cosmos.container\", \"IoTSignals\")\\\n",
					"                    .load()"
				],
				"execution_count": 29
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Data Exploration using Pyplot\n",
					"\n",
					"\n",
					"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/DataExPlot.png\" alt=\"Chart\" width=\"75%\"/>"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd\n",
					"import matplotlib.pyplot as plt\n",
					"\n",
					"df_IoTSignals_pd = df_IoTSignals.toPandas().dropna()\n",
					"df_IoTSignals_pd['measureValue'] = df_IoTSignals_pd['measureValue'].astype(int)\n",
					"\n",
					"df_MW = df_IoTSignals_pd[(df_IoTSignals_pd['deviceId'] == 'dev-1') & (df_IoTSignals_pd['unitSymbol'] == 'MW')]\n",
					"df_MW.plot(x='dateTime', y='measureValue', color='green', figsize=(20,5), label = 'Output MW')\n",
					"plt.title('MW TimeSeries')\n",
					"plt.show()\n",
					"\n",
					"df_RPM = df_IoTSignals_pd[(df_IoTSignals_pd['deviceId'] == 'dev-1') & (df_IoTSignals_pd['unitSymbol'] == 'RPM')]\n",
					"df_RPM.plot(x='dateTime', y='measureValue', color='black', figsize=(20,5), label = 'Output RPM')\n",
					"plt.title('RPM TimeSeries')\n",
					"plt.show()"
				],
				"execution_count": 30
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Perform Anomaly Detection using Microsoft Machine Learning for Spark (MMLSpark)"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col\n",
					"from synapse.ml.services.anomaly import SimpleDetectAnomalies\n",
					"\n",
					"# 1) Configure the anomaly detector (uses synapse.ml.services, not mmlspark)\n",
					"anomaly_detector = (\n",
					"    SimpleDetectAnomalies()\n",
					"      .setSubscriptionKey(Cognitive_Services_Key)\n",
					"      .setLocation(\"southcentralus\")  # adjust if needed\n",
					"      .setOutputCol(\"anomalies\")\n",
					"      .setGroupbyCol(\"grouping\")\n",
					"      .setSensitivity(95)\n",
					"      .setGranularity(\"secondly\")\n",
					")\n",
					"\n",
					"# 2) Prepare your signal DataFrame\n",
					"signals_df = (\n",
					"    df_IoTSignals\n",
					"      .where(col(\"unitSymbol\") == \"RPM\")\n",
					"      .withColumnRenamed(\"dateTime\", \"timestamp\")\n",
					"      .withColumn(\"value\", col(\"measureValue\").cast(\"double\"))\n",
					"      .withColumn(\"grouping\", col(\"deviceId\"))\n",
					")\n",
					"\n",
					"# 3) Run the detector\n",
					"df_anomaly = anomaly_detector.transform(signals_df)\n",
					"\n",
					"# 4) Register for SQL queries\n",
					"df_anomaly.createOrReplaceTempView(\"df_anomaly\")\n",
					"\n",
					"print(\"▶️ Anomaly detection complete; temp view 'df_anomaly' is ready.\")\n",
					""
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"source": [
					"# from pyspark.sql.functions import col\n",
					"# from pyspark.sql.types import *\n",
					"# from mmlspark.cognitive import SimpleDetectAnomalies\n",
					"# from mmlspark.core.spark import FluentAPI\n",
					"\n",
					"# anomaly_detector = (SimpleDetectAnomalies()\n",
					"#                             .setSubscriptionKey(Cognitive_Services_Key)\n",
					"#                             .setLocation('southcentralus') ##Optional, Update If Required\n",
					"#                             .setOutputCol(\"anomalies\")\n",
					"#                             .setGroupbyCol(\"grouping\")\n",
					"#                             .setSensitivity(95)\n",
					"#                             .setGranularity(\"secondly\"))\n",
					"\n",
					"# df_anomaly = (df_IoTSignals\n",
					"#                     .where(col(\"unitSymbol\") == 'RPM')\n",
					"#                     .withColumnRenamed(\"dateTime\", \"timestamp\")\n",
					"#                     .withColumn(\"value\", col(\"measureValue\").cast(\"double\"))\n",
					"#                     .withColumn(\"grouping\", col(\"deviceId\"))\n",
					"#                     .mlTransform(anomaly_detector))\n",
					"\n",
					"# df_anomaly.createOrReplaceTempView('df_anomaly')"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(df_anomaly)"
				],
				"execution_count": 33
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Format the Dataframe for Visualization\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df_anomaly_single_device = spark.sql(\"select timestamp \\\n",
					"                                            , measureValue \\\n",
					"                                            , anomalies.expectedValue \\\n",
					"                                            , anomalies.expectedValue + anomalies.upperMargin as expectedUpperValue \\\n",
					"                                            , anomalies.expectedValue - anomalies.lowerMargin as expectedLowerValue \\\n",
					"                                            , case when anomalies.isAnomaly=true then 1 else 0 end as isAnomaly \\\n",
					"                                        from df_anomaly \\\n",
					"                                        where deviceid = 'dev-1' \\\n",
					"                                        order by timestamp \\\n",
					"                                        limit 400\")\n",
					"\n",
					"display(df_anomaly_single_device)"
				],
				"execution_count": 34
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Visualize the Anomalies using Plotly\n",
					"Plot Expected value, Upper Value, Lower Value and Actual Value along with Anomaly flag\n",
					"\n",
					"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/AnmaliesPlot.png\" alt=\"Chart\" width=\"75%\"/>\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import warnings\n",
					"warnings.filterwarnings(\n",
					"    \"ignore\",\n",
					"    category=DeprecationWarning,\n",
					"    module=\"pyspark.sql.pandas.utils\"\n",
					")\n",
					"\n",
					"import matplotlib.pyplot as plt\n",
					"from pyspark.sql.functions import col\n",
					"\n",
					"# Assuming df_anomaly_single_device is already defined\n",
					"adf = df_anomaly_single_device.toPandas()\n",
					"adf_subset = adf[adf['isAnomaly'] == 1]\n",
					"\n",
					"plt.figure(figsize=(23, 8))\n",
					"plt.plot(\n",
					"    adf['timestamp'],\n",
					"    adf['expectedUpperValue'],\n",
					"    color='darkred',\n",
					"    linestyle='solid',\n",
					"    linewidth=0.25\n",
					")\n",
					"plt.plot(\n",
					"    adf['timestamp'],\n",
					"    adf['expectedValue'],\n",
					"    color='darkgreen',\n",
					"    linestyle='solid',\n",
					"    linewidth=2\n",
					")\n",
					"# Removed the 'b' format string to avoid redundant color warning\n",
					"plt.plot(\n",
					"    adf['timestamp'],\n",
					"    adf['measureValue'],\n",
					"    color='royalblue',\n",
					"    linestyle='dotted',\n",
					"    linewidth=2\n",
					")\n",
					"plt.plot(\n",
					"    adf['timestamp'],\n",
					"    adf['expectedLowerValue'],\n",
					"    color='black',\n",
					"    linestyle='solid',\n",
					"    linewidth=0.25\n",
					")\n",
					"# Use shorthand 'ro' (red circle) without extra color kw\n",
					"plt.plot(\n",
					"    adf_subset['timestamp'],\n",
					"    adf_subset['measureValue'],\n",
					"    'ro'\n",
					")\n",
					"\n",
					"plt.legend([\n",
					"    'RPM-UpperMargin',\n",
					"    'RPM-ExpectedValue',\n",
					"    'RPM-ActualValue',\n",
					"    'RPM-LowerMargin',\n",
					"    'RPM-Anomaly'\n",
					"])\n",
					"plt.title('RPM Anomalies with Expected, Actual, Upper and Lower Values')\n",
					"plt.show()\n",
					""
				],
				"execution_count": 35
			}
		]
	}
}