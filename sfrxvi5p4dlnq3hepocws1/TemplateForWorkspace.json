{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "sfrxvi5p4dlnq3hepocws1"
		},
		"CosmosDBLink_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'CosmosDBLink'"
		},
		"keyVaultLinkedservice_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "@{concat('https://',linkedService().keyVaultName,'.vault.azure.net/')}"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/CosmosDBLink')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDb",
				"typeProperties": {
					"connectionString": "[parameters('CosmosDBLink_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/keyVaultLinkedservice')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"keyVaultName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('keyVaultLinkedservice_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0,
							"cleanup": true
						}
					}
				},
				"managedVirtualNetwork": {
					"type": "ManagedVirtualNetworkReference",
					"referenceName": "default"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks",
			"apiVersion": "2019-06-01-preview",
			"properties": {},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/1-SalesForecastingWithAML')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "ws1sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "38",
						"spark.autotune.trackingId": "35a19baa-a8e7-4720-af90-2549bfa215db"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/41918b4b-7cb5-4e49-a80a-8ef16287837d/resourceGroups/ML-SalesForecast-Retail/providers/Microsoft.Synapse/workspaces/sfrxvi5p4dlnq3hepocws1/bigDataPools/ws1sparkpool1",
						"name": "ws1sparkpool1",
						"type": "Spark",
						"endpoint": "https://sfrxvi5p4dlnq3hepocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ws1sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 5,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Leverage Power of Azure Synapse Link for Cosmos DB and Spark SQL\n",
							"With Synapse Link, you can now directly connect to your Azure Cosmos DB containers from Azure Synapse Analytics and access the analytical store with no separate connectors. This notebook scenario is to \n",
							"- Ingest data into Cosmos DB containers using Azure Synapse Spark\n",
							"- Setup Spark tables\n",
							"- Join & aggregate operational data across Cosmos DB containers\n",
							"- Perform Near real-time Sales Forecasting using Azure Automated Machine Learning using Synapse Link for Cosmos DB"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/synapse-cosmosdb.png\" alt=\"Surface Device\" width=\"75%\"/>\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# ──────────────────────────────────────────────────────────────────────────\n",
							"# Cell 1 — install & pin all dependencies before any imports\n",
							"# ──────────────────────────────────────────────────────────────────────────\n",
							"\n",
							"# Key Vault client libs\n",
							"%pip install --quiet azure-keyvault-secrets azure-identity\n",
							"\n",
							"# Azure ML SDK v1 for AutoML + core\n",
							"%pip install --quiet azureml-train-automl azureml-core\n",
							"\n",
							"# Pin NumPy to the version AutoML expects\n",
							"%pip install --quiet \"numpy>=1.16.0,<=1.23.5\"\n",
							"\n",
							"# (Optional) any other packages you know you'll need, e.g. pandas, scikit-learn, etc.\n",
							"# %pip install --quiet pandas scikit-learn\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": true
							}
						},
						"source": [
							"import os, logging\n",
							"import pandas as pd\n",
							"from azureml.core import Workspace, Experiment, Dataset, Environment\n",
							"from azureml.core.keyvault import Keyvault\n",
							"from azureml.train.automl import AutoMLConfig\n",
							"from azure.keyvault.secrets import SecretClient\n",
							"from azure.identity import DefaultAzureCredential\n",
							"# …etc…\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 101
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Replace all \"AML\" placeholders with your own information,Replace subscription_id , resource_group with values before execution\n",
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/SF_Ws_Variables.gif\" alt=\"Surface Device\" width=\"75%\"/>\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"AMLWsname='sfr-mlws-dev01'\n",
							"AMLWssubscription_id='41918b4b-7cb5-4e49-a80a-8ef16287837d'\n",
							"AMLWsresource_group='ML-SalesForecast-Retail'"
						],
						"outputs": [],
						"execution_count": 102
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Data Ingestion into Cosmos DB using Synapse Cosmos DB Link"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"\n",
							"# 1) List of public CSV URLs\n",
							"urls = [\n",
							"    \"https://raw.githubusercontent.com/Azure-Samples/Synapse/\"\n",
							"    \"main/Notebooks/PySpark/Synapse%20Link%20for%20Cosmos%20DB%20samples/\"\n",
							"    \"Retail/RetailData/Products.csv\",\n",
							"    \"https://raw.githubusercontent.com/Azure-Samples/Synapse/\"\n",
							"    \"main/Notebooks/PySpark/Synapse%20Link%20for%20Cosmos%20DB%20samples/\"\n",
							"    \"Retail/RetailData/RetailSales.csv\",\n",
							"    \"https://raw.githubusercontent.com/Azure-Samples/Synapse/\"\n",
							"    \"main/Notebooks/PySpark/Synapse%20Link%20for%20Cosmos%20DB%20samples/\"\n",
							"    \"Retail/RetailData/StoreDemoGraphics.csv\"\n",
							"]\n",
							"\n",
							"for url in urls:\n",
							"    # 2) Derive 'Products', 'RetailSales', 'StoreDemoGraphics'\n",
							"    filename = os.path.basename(url)\n",
							"    container_name = os.path.splitext(filename)[0]\n",
							"\n",
							"    print(f\"▶️ Downloading & loading '{filename}'…\")\n",
							"    pdf = pd.read_csv(url)                   # driver‑side fetch\n",
							"\n",
							"    print(f\"✉️ Writing into Cosmos container '{container_name}'…\")\n",
							"    sdf = spark.createDataFrame(pdf)         # Spark DF\n",
							"\n",
							"    (\n",
							"        sdf.write\n",
							"           .format(\"cosmos.oltp\")\n",
							"           .option(\"spark.synapse.linkedService\", \"CosmosDBLink\")\n",
							"           .option(\"spark.cosmos.container\", container_name)\n",
							"           .option(\"spark.cosmos.write.strategy\", \"ItemOverwrite\")  # uses upsert :contentReference[oaicite:1]{index=1}\n",
							"           .mode(\"append\")\n",
							"           .save()\n",
							"    )\n",
							"\n",
							"print(\"✅ All retail datasets ingested into Cosmos.\")\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 103
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create Spark tables pointing to the Azure Cosmos DB Analytical Store collections using Azure Synapse Link"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"create database if not exists RetailSalesDB;\n",
							"\n",
							"create table if not exists RetailSalesDB.RetailSales using cosmos.olap options (\n",
							"    spark.synapse.linkedService 'CosmosDBLink',\n",
							"    spark.cosmos.preferredRegions 'East US',  --Optional, Update If Required\n",
							"    spark.cosmos.container 'RetailSales'\n",
							");\n",
							"\n",
							"create table if not exists RetailSalesDB.StoreDemographics using cosmos.olap options (\n",
							"    spark.synapse.linkedService 'CosmosDBLink',\n",
							"    spark.cosmos.preferredRegions 'East US', --Optional, Update If Required\n",
							"    spark.cosmos.container 'StoreDemoGraphics'\n",
							");\n",
							"create table if not exists RetailSalesDB.Product using cosmos.olap options (\n",
							"    spark.synapse.linkedService 'CosmosDBLink',\n",
							"    spark.cosmos.preferredRegions 'East US', --Optional, Update If Required\n",
							"    spark.cosmos.container 'Products'\n",
							");"
						],
						"outputs": [],
						"execution_count": 104
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Perform Joins across collections, apply filters and aggregations using Spark SQL"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"data = spark.sql(\"select a.storeId \\\n",
							"                       , b.productCode \\\n",
							"                       , b.wholeSaleCost \\\n",
							"                       , b.basePrice \\\n",
							"                       , c.ratioAge60 \\\n",
							"                       , c.collegeRatio \\\n",
							"                       , c.income \\\n",
							"                       , c.highIncome150Ratio \\\n",
							"                       , c.largeHH \\\n",
							"                       , c.minoritiesRatio \\\n",
							"                       , c.more1FullTimeEmployeeRatio \\\n",
							"                       , c.distanceNearestWarehouse \\\n",
							"                       , c.salesNearestWarehousesRatio \\\n",
							"                       , c.avgDistanceNearest5Supermarkets \\\n",
							"                       , c.salesNearest5StoresRatio \\\n",
							"                       , a.quantity \\\n",
							"                       , a.logQuantity \\\n",
							"                       , a.advertising \\\n",
							"                       , a.price \\\n",
							"                       , a.weekStarting \\\n",
							"                 from RetailSalesDB.retailsales a \\\n",
							"                 left join RetailSalesDB.product b \\\n",
							"                 on a.productcode = b.productcode \\\n",
							"                 left join RetailSalesDB.storedemographics c \\\n",
							"                 on a.storeId = c.storeId \\\n",
							"                 order by a.weekStarting, a.storeId, b.productCode\")\n",
							"\n",
							"display(data)"
						],
						"outputs": [],
						"execution_count": 105
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Predictive Analytics\n",
							"Leverage power of Azure Machine Learning's AutoML to build a Forecasting Model Predictive analytics can help us to study and discover the factors that determine the number of sales that a retail store will have in the future.This notebook scenario is [Microsoft Surface](https://www.microsoft.com/en-us/surface) sales forecasting, with artificially created data. The business challenge is a distributor that wants to predict how many units are necessary in the local warehouse to supply the stores in the area.\n",
							"We will use Quantitative Models to forecast future data as a function of past data. They are appropriate to use when past numerical data is available and when it is reasonable to assume that some of the patterns in the data are expected to continue into the future. These methods are usually applied to short or intermediate range decisions. For more information, click [here](https://en.wikipedia.org/wiki/Forecasting).\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Only run this the first time to create the workspace\n",
							"\n",
							"# from azureml.core import Workspace\n",
							"\n",
							"# ws = Workspace.create(name=AMLWsname,\n",
							"#                subscription_id=AMLWssubscription_id,\n",
							"#                resource_group=AMLWsresource_group,\n",
							"#                create_resource_group=True,\n",
							"#                location='East US' ## Optional, Update If Required\n",
							"#                )"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"\n",
							"# Pull your existing workspace by name / subscription / resource‑group:\n",
							"ws = Workspace.get(\n",
							"    name               = AMLWsname,\n",
							"    subscription_id    = AMLWssubscription_id,\n",
							"    resource_group     = AMLWsresource_group\n",
							")\n",
							"\n",
							"print(f\"Connected to workspace {ws.name} in {ws.location}\")\n",
							""
						],
						"outputs": [],
						"execution_count": 106
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"\n",
							"\n",
							"# 1) Get your workspace’s default key vault\n",
							"kv = Keyvault(workspace=ws)\n",
							"\n",
							"# 2) Pull each secret by name\n",
							"client_id     = kv.get_secret(\"sp-client-id\")\n",
							"client_secret = kv.get_secret(\"sp-client-secret\")\n",
							"tenant_id     = kv.get_secret(\"sp-tenant-id\")\n",
							"\n",
							"# 3) Populate the env‑vars so DefaultAzureCredential picks them up\n",
							"os.environ[\"AZURE_CLIENT_ID\"]     = client_id\n",
							"os.environ[\"AZURE_CLIENT_SECRET\"] = client_secret\n",
							"os.environ[\"AZURE_TENANT_ID\"]     = tenant_id\n",
							"\n",
							"print(\"✅ Loaded SP credentials into environment from Key Vault.\")\n",
							""
						],
						"outputs": [],
						"execution_count": 107
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# import azureml.core\n",
							"# import pandas as pd\n",
							"# import numpy as np\n",
							"# import logging\n",
							"# from azureml.core.workspace import Workspace\n",
							"# from azureml.core import Workspace\n",
							"# from azureml.core.experiment import Experiment\n",
							"# from azureml.train.automl import AutoMLConfig\n",
							"# import os\n",
							"# subscription_id = ws.subscription_id\n",
							"# resource_group = ws.resource_group\n",
							"# workspace_name = ws._workspace_name\n",
							"# workspace_region = 'East US' ## Optional, Update If Required\n",
							"# ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
							"# ws.write_config()\n",
							"    \n",
							"# experiment_name = 'automl-surfaceforecasting'\n",
							"# experiment = Experiment(ws, experiment_name)\n",
							"# output = {}\n",
							"# output['Subscription ID'] = ws.subscription_id\n",
							"# output['Workspace'] = ws.name\n",
							"# output['SKU'] = ws.sku\n",
							"# output['Resource Group'] = ws.resource_group\n",
							"# output['Location'] = ws.location\n",
							"# output['Run History Name'] = experiment_name\n",
							"# pd.set_option('display.max_colwidth', None)\n",
							"# outputDf = pd.DataFrame(data = output, index = [''])\n",
							"# outputDf.T"
						],
						"outputs": [],
						"execution_count": 84
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Data Preparation - Feature engineering, Splitting train & test datasets\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Initial variables\n",
							"time_column_name = 'weekStarting'\n",
							"grain_column_names = ['storeId', 'productCode']\n",
							"target_column_name = 'quantity'\n",
							"use_stores = [2, 5, 8,71,102]\n",
							"n_test_periods = 20\n",
							"\n",
							"\n",
							"#DataFrame\n",
							"df = data.toPandas()\n",
							"df[time_column_name] = pd.to_datetime(df[time_column_name])\n",
							"df['storeId'] = pd.to_numeric(df['storeId'])\n",
							"df['quantity'] = pd.to_numeric(df['quantity'])\n",
							"df['advertising'] = pd.to_numeric(df['advertising'])\n",
							"df['price'] = df['price'].astype(float)\n",
							"df['basePrice'] = df['basePrice'].astype(float)\n",
							"df['ratioAge60'] = df['ratioAge60'].astype(float)\n",
							"df['collegeRatio'] = df['collegeRatio'].astype(float)\n",
							"df['highIncome150Ratio'] = df['highIncome150Ratio'].astype(float)\n",
							"df['income'] = df['income'].astype(float)\n",
							"df['largeHH'] = df['largeHH'].astype(float)\n",
							"df['minoritiesRatio'] = df['minoritiesRatio'].astype(float)\n",
							"df['logQuantity'] = df['logQuantity'].astype(float)\n",
							"df['more1FullTimeEmployeeRatio'] = df['more1FullTimeEmployeeRatio'].astype(float)\n",
							"df['distanceNearestWarehouse'] = df['distanceNearestWarehouse'].astype(float)\n",
							"df['salesNearestWarehousesRatio'] = df['salesNearestWarehousesRatio'].astype(float)\n",
							"df['avgDistanceNearest5Supermarkets'] = df['avgDistanceNearest5Supermarkets'].astype(float)\n",
							"df['salesNearest5StoresRatio'] = df['salesNearest5StoresRatio'].astype(float)\n",
							"\n",
							"\n",
							"# Time Series\n",
							"data_subset = df[df.storeId.isin(use_stores)]\n",
							"nseries = data_subset.groupby(grain_column_names).ngroups\n",
							"print('Data subset contains {0} individual time-series.'.format(nseries))\n",
							"\n",
							"# Group by date\n",
							"def split_last_n_by_grain(df, n):\n",
							"    \"\"\"Group df by grain and split on last n rows for each group.\"\"\"\n",
							"    df_grouped = (df.sort_values(time_column_name) # Sort by ascending time\n",
							"                  .groupby(grain_column_names, group_keys=False))\n",
							"    df_head = df_grouped.apply(lambda dfg: dfg.iloc[:-n])\n",
							"    df_tail = df_grouped.apply(lambda dfg: dfg.iloc[-n:])\n",
							"    return df_head, df_tail\n",
							"\n",
							"# splitting\n",
							"train, test = split_last_n_by_grain(data_subset, n_test_periods)\n",
							"print(len(train),len(test))\n",
							"train.to_csv (r'./SurfaceSales_train.csv', index = None, header=True)\n",
							"test.to_csv (r'./SurfaceSales_test.csv', index = None, header=True)\n",
							"datastore = ws.get_default_datastore()\n",
							"datastore.upload_files(files = ['./SurfaceSales_train.csv', './SurfaceSales_test.csv'], target_path = 'dataset/', overwrite = True,show_progress = True)\n",
							"\n",
							"# loading the train dataset\n",
							"from azureml.core.dataset import Dataset\n",
							"train_dataset = Dataset.Tabular.from_delimited_files(path=datastore.path('dataset/SurfaceSales_train.csv'))"
						],
						"outputs": [],
						"execution_count": 108
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create custom environment for the run"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"\n",
							"time_series_settings = {\n",
							"    \"time_column_name\":   time_column_name,\n",
							"    \"grain_column_names\": grain_column_names,\n",
							"    \"max_horizon\":        n_test_periods\n",
							"}\n",
							"\n",
							"automl_config = AutoMLConfig(\n",
							"    task=\"forecasting\",\n",
							"    debug_log=\"automl_ss_sales_errors.log\",\n",
							"    primary_metric=\"normalized_mean_absolute_error\",\n",
							"    experiment_timeout_minutes=30,\n",
							"    training_data=train_dataset,\n",
							"    label_column_name=target_column_name,\n",
							"    enable_early_stopping=True,\n",
							"    n_cross_validations=3,\n",
							"    iterations=15,\n",
							"    verbosity=logging.INFO,\n",
							"    **time_series_settings\n",
							")\n",
							"\n",
							"experiment = Experiment(ws, \"automl-surfaceforecasting\")\n",
							"run = experiment.submit(automl_config, show_output=True)\n",
							"run.wait_for_completion(show_output=True)\n",
							""
						],
						"outputs": [],
						"execution_count": 111
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from azureml.train.automl import AutoMLConfig\n",
							"from azureml.core import Experiment\n",
							"import logging\n",
							"\n",
							"# Time‑series parameters\n",
							"time_series_settings = {\n",
							"    \"time_column_name\":   time_column_name,\n",
							"    \"grain_column_names\": grain_column_names,\n",
							"    \"max_horizon\":        n_test_periods\n",
							"}\n",
							"\n",
							"# 1) Build the AutoMLConfig for forecasting\n",
							"automl_config = AutoMLConfig(\n",
							"    task=\"forecasting\",\n",
							"    primary_metric=\"normalized_mean_absolute_error\",\n",
							"    experiment_timeout_minutes=30,\n",
							"    training_data=train_dataset,\n",
							"    label_column_name=target_column_name,\n",
							"    enable_early_stopping=True,\n",
							"    n_cross_validations=3,\n",
							"    iterations=15,\n",
							"    verbosity=logging.INFO,\n",
							"    **time_series_settings\n",
							")\n",
							"\n",
							"# 2) Submit the experiment (using the same `ws` you loaded earlier)\n",
							"experiment = Experiment(ws, \"automl-surfaceforecasting\")\n",
							"remote_run = experiment.submit(automl_config, show_output=True)\n",
							"\n",
							"# (Optional) wait for it to finish and view metrics\n",
							"remote_run.wait_for_completion(show_output=True)\n",
							""
						],
						"outputs": [],
						"execution_count": 73
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create the ML Client"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from azure.identity import DefaultAzureCredential\n",
							"from azure.ai.ml import MLClient\n",
							"\n",
							"# 0) Authenticate via the env‑vars you set (AZURE_CLIENT_ID, SECRET, TENANT)\n",
							"credential = DefaultAzureCredential()\n",
							"\n",
							"# 1) Instantiate the MLClient (reads subscription, RG, WS from config.json)\n",
							"ml_client = MLClient.from_config(credential)\n",
							""
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Training the Models using AutoML Forecasting\n",
							"\n",
							"Please notice that **compute_target** is commented, meaning that the model training will run locally in Synapse Spark."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from azure.ai.ml import Input\n",
							"from azure.ai.ml.automl import ForecastingJob\n",
							"\n",
							"# 1) Define your training file as an Input\n",
							"train_data_input = Input(\n",
							"    type=\"uri_file\",  # since it’s a flat CSV\n",
							"    path=\"azureml://datastores/workspaceblobstore/paths/dataset/SurfaceSales_train.csv\"\n",
							")\n",
							"\n",
							"# 2) Build the ForecastingJob\n",
							"job = ForecastingJob(\n",
							"    compute=\"cpu-cluster\",\n",
							"    display_name=\"sales-forecasting\",\n",
							"    experiment_name=\"automl-surfaceforecasting\",\n",
							"    training_data=train_data_input,            # <-- use Input here\n",
							"    target_column_name=\"quantity\",\n",
							"    primary_metric=\"normalized_mean_absolute_error\"\n",
							")\n",
							"\n",
							"# 3) Time-series settings\n",
							"job.set_forecast_settings(\n",
							"    time_column_name=\"weekStarting\",\n",
							"    forecast_horizon=20,\n",
							"    time_series_id_column_names=[\"storeId\", \"productCode\"]\n",
							")\n",
							"\n",
							"# 4) Limits (including early stopping)\n",
							"job.set_limits(\n",
							"    timeout_minutes=30,\n",
							"    trial_timeout_minutes=10,\n",
							"    max_concurrent_trials=2,\n",
							"    max_trials=15,\n",
							"    enable_early_termination=True\n",
							")\n",
							"\n",
							"# 5) Training options\n",
							"#job.set_training()\n",
							"\n",
							"# 6) Submit and stream\n",
							"returned_job = ml_client.jobs.create_or_update(job)\n",
							"ml_client.jobs.stream(returned_job.name)\n",
							""
						],
						"outputs": [],
						"execution_count": 72
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Parameters\n",
							"time_series_settings = {\n",
							"    'time_column_name': time_column_name,\n",
							"    'grain_column_names': grain_column_names,\n",
							"    'max_horizon': n_test_periods\n",
							"}\n",
							"\n",
							"# Config\n",
							"automl_config = AutoMLConfig(task='forecasting',\n",
							"                             debug_log='automl_ss_sales_errors.log',\n",
							"                             primary_metric='normalized_mean_absolute_error',\n",
							"                             experiment_timeout_hours=0.5,\n",
							"                             training_data=train_dataset,\n",
							"                             label_column_name=target_column_name,\n",
							"                             #compute_target=compute_target,\n",
							"                             enable_early_stopping=True,\n",
							"                             n_cross_validations=3,\n",
							"                             verbosity=logging.INFO,\n",
							"                             iterations=15,\n",
							"                             **time_series_settings)\n",
							"\n",
							"# Running the training\n",
							"remote_run = experiment.submit(automl_config, show_output=True)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Retrieving the Best Model and Forecasting"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Retrieving the best model\n",
							"best_run, fitted_model = remote_run.get_output()\n",
							"print(fitted_model.steps)\n",
							"model_name = best_run.properties['model_name']\n",
							"print(model_name)\n",
							"\n",
							"# Forecasting based on test dataset\n",
							"X_test = test\n",
							"y_test = X_test.pop(target_column_name).values\n",
							"X_test[time_column_name] = pd.to_datetime(X_test[time_column_name])\n",
							"y_predictions, X_trans = fitted_model.forecast(X_test)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Plotting the Results"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import pandas as pd\n",
							"import numpy as np\n",
							"from pandas.tseries.frequencies import to_offset\n",
							"\n",
							"\n",
							"def align_outputs(y_predicted, X_trans, X_test, y_test, target_column_name,\n",
							"                  predicted_column_name='predicted',\n",
							"                  horizon_colname='horizon_origin'):\n",
							"    \"\"\"\n",
							"    Demonstrates how to get the output aligned to the inputs\n",
							"    using pandas indexes. Helps understand what happened if\n",
							"    the output's shape differs from the input shape, or if\n",
							"    the data got re-sorted by time and grain during forecasting.\n",
							"\n",
							"    Typical causes of misalignment are:\n",
							"    * we predicted some periods that were missing in actuals -> drop from eval\n",
							"    * model was asked to predict past max_horizon -> increase max horizon\n",
							"    * data at start of X_test was needed for lags -> provide previous periods\n",
							"    \"\"\"\n",
							"\n",
							"    if (horizon_colname in X_trans):\n",
							"        df_fcst = pd.DataFrame({predicted_column_name: y_predicted,\n",
							"                                horizon_colname: X_trans[horizon_colname]})\n",
							"    else:\n",
							"        df_fcst = pd.DataFrame({predicted_column_name: y_predicted})\n",
							"\n",
							"    # y and X outputs are aligned by forecast() function contract\n",
							"    df_fcst.index = X_trans.index\n",
							"\n",
							"    # align original X_test to y_test\n",
							"    X_test_full = X_test.copy()\n",
							"    X_test_full[target_column_name] = y_test\n",
							"\n",
							"    # X_test_full's index does not include origin, so reset for merge\n",
							"    df_fcst.reset_index(inplace=True)\n",
							"    X_test_full = X_test_full.reset_index().drop(columns='index')\n",
							"    together = df_fcst.merge(X_test_full, how='right')\n",
							"\n",
							"    # drop rows where prediction or actuals are nan\n",
							"    # happens because of missing actuals\n",
							"    # or at edges of time due to lags/rolling windows\n",
							"    clean = together[together[[target_column_name,\n",
							"                               predicted_column_name]].notnull().all(axis=1)]\n",
							"    return(clean)\n",
							"\n",
							"\n",
							"df_all = align_outputs(y_predictions, X_trans, X_test, y_test, target_column_name)\n",
							"\n",
							"#from azureml.automl.core._vendor.automl.client.core.common import metrics\n",
							"from matplotlib import pyplot as plt\n",
							"from automl.client.core.common import constants\n",
							"\n",
							"from azureml.automl.runtime.shared.score import scoring\n",
							"scores = scoring.score_regression(\n",
							"    y_test=df_all[target_column_name],\n",
							"    y_pred=df_all['predicted'],\n",
							"    metrics=list(constants.Metric.SCALAR_REGRESSION_SET))\n",
							"# use automl metrics module\n",
							"#scores = metrics.compute_metrics_regression(\n",
							" #   df_all['predicted'],\n",
							"  #  df_all[target_column_name],\n",
							"   # list(constants.Metric.SCALAR_REGRESSION_SET),\n",
							"    #None, None, None)\n",
							"\n",
							"print(\"[Test data scores]\\n\")\n",
							"for key, value in scores.items():    \n",
							"    print('{}:   {:.3f}'.format(key, value))\n",
							"    \n",
							"# Plot outputs\n",
							"#%matplotlib inline\n",
							"test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\n",
							"test_test = plt.scatter(df_all[target_column_name], df_all[target_column_name], color='g')\n",
							"plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Closing\n",
							"\n",
							"At this point you should have a chart like this one in the image below, created wit AutoML and MatplotLib. The results are that good because of the **logQuantity** column, a  data Leakage calculated from que **quantity** column. You can try to run the same experiment without it.\n",
							"\n",
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/prediction.png\" alt=\"Chart\" width=\"75%\"/>\n",
							"\n",
							""
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/2-AnomalyDetectionWithMML')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "ws1sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 19,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "19",
						"spark.dynamicAllocation.maxExecutors": "19",
						"spark.autotune.trackingId": "5d01f62a-3816-4660-801d-0d961e4b45d0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/41918b4b-7cb5-4e49-a80a-8ef16287837d/resourceGroups/ML-SalesForecast-Retail/providers/Microsoft.Synapse/workspaces/sfrxvi5p4dlnq3hepocws1/bigDataPools/ws1sparkpool1",
						"name": "ws1sparkpool1",
						"type": "Spark",
						"endpoint": "https://sfrxvi5p4dlnq3hepocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ws1sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 5,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Leverage Power of Azure Synapse Link for Cosmos DB and Spark SQL\n",
							"With Synapse Link, you can now directly connect to your Azure Cosmos DB containers from Azure Synapse Analytics and access the analytical store with no separate connectors. This notebook scenario is to \n",
							"\n",
							"- Ingest streaming data into Azure Cosmos DB collection using Structured Streaming\n",
							"- Ingest Batch data into Azure Cosmos DB collection using Azure Synapse Spark\n",
							"- Format the stream dataframe as per the IoTSignals schema\n",
							"- Write the streaming dataframe to the Azure Cosmos DB collection\n",
							"- Perform Joins and aggregations across Azure Cosmos DB collections using Azure Synapse Link\n",
							"- Perform Anomaly Detection using Azure Synapse Link and Azure Cognitive Services on Synapse Spark (MMLSpark)"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/synapse-cosmosdb.png\" alt=\"Surface Device\" width=\"75%\"/>\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Replace the Cognitive Services API account Keys below before execution.Keys can be found under \"Keys and Endpoints\" Section\n",
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/AD_Ws_Varriables.gif\" alt=\"Surface Device\" width=\"75%\"/>\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Cognitive_Services_Key='d2697b0a802b45bea7c8c8969320daa1'"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Simulate Streaming Data Generation using Rate Streaming Source\n",
							"The Rate streaming source is used to simplify the solution here and can be replaced with any supported streaming sources such as Azure Event Hubs and Apache Kafka."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"dfStream = (spark\n",
							"                .readStream\n",
							"                .format(\"rate\")\n",
							"                .option(\"rowsPerSecond\", 10)\n",
							"                .load()\n",
							"            )"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Format the Stream Dataframe as per the IoTSignals Schema"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"import pyspark.sql.functions as F\n",
							"from pyspark.sql.types import StringType\n",
							"import uuid\n",
							"\n",
							"numberOfDevices = 10\n",
							"generate_uuid = F.udf(lambda : str(uuid.uuid4()), StringType())\n",
							"              \n",
							"dfIoTSignals = (dfStream\n",
							"                    .withColumn(\"id\", generate_uuid())\n",
							"                    .withColumn(\"deviceId\", F.concat(F.lit(\"dev-\"), F.expr(\"mod(value, %d)\" % numberOfDevices)))\n",
							"                    .withColumn(\"dateTime\", dfStream[\"timestamp\"].cast(StringType()))\n",
							"                    .withColumn(\"unit\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Revolutions per Minute' ELSE 'MegaWatts' END\"))\n",
							"                    .withColumn(\"unitSymbol\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'RPM' ELSE 'MW' END\"))\n",
							"                    .withColumn(\"measureType\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Rotation Speed' ELSE 'Output' END\"))\n",
							"                    .withColumn(\"measureValue\", F.expr(\"CASE WHEN rand() > 0.95 THEN value * 10 WHEN rand() < 0.05 THEN value div 10 ELSE value END\"))\n",
							"                    .drop(\"timestamp\", \"value\")\n",
							"                )"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Stream Writes to the Azure Cosmos DB Collection\n",
							"The \"cosmos.oltp\" is the Spark format that enables connection to the Cosmos DB Transactional store."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Stream IoTSignals into Cosmos DB via Synapse Link\n",
							"streamQuery = (\n",
							"    dfIoTSignals\n",
							"      .writeStream\n",
							"      .format(\"cosmos.oltp\")\n",
							"      .outputMode(\"append\")\n",
							"      .option(\"spark.synapse.linkedService\", \"CosmosDBLink\")\n",
							"      .option(\"spark.cosmos.container\", \"IoTSignals\")\n",
							"      .option(\"spark.cosmos.write.strategy\", \"ItemOverwrite\")  # performs upsert \n",
							"      .option(\"checkpointLocation\", \"/writeCheckpointDir\")\n",
							"      .start()\n",
							")\n",
							"\n",
							"# Let it run for up to 3 minutes, then stop\n",
							"streamQuery.awaitTermination(180)\n",
							"streamQuery.stop()\n",
							""
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Load the IoTDeviceInfo Dataset from ADLS Gen2 to a Dataframe and Write the Dataframe to the Azure Cosmos DB Collection\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import pandas as pd\n",
							"from pyspark.sql.functions import col\n",
							"\n",
							"# 1) Pull IoTDeviceInfo into pandas\n",
							"url = (\n",
							"    \"https://raw.githubusercontent.com/Azure-Samples/Synapse/\"\n",
							"    \"main/Notebooks/PySpark/Synapse%20Link%20for%20Cosmos%20DB%20samples/\"\n",
							"    \"IoT/IoTData/IoTDeviceInfo.csv\"\n",
							")\n",
							"print(\"▶️ Downloading IoTDeviceInfo.csv…\")\n",
							"pdf = pd.read_csv(url)\n",
							"\n",
							"# 2) Convert to Spark DF and add an 'id' column from 'deviceid'\n",
							"sdf = (\n",
							"    spark\n",
							"      .createDataFrame(pdf)\n",
							"      .withColumn(\"id\", col(\"deviceid\").cast(\"string\"))\n",
							")\n",
							"\n",
							"# 3) Write into Cosmos with upsert strategy\n",
							"print(\"✉️ Writing into Cosmos container 'IoTDeviceInfo'…\")\n",
							"(\n",
							"    sdf.write\n",
							"       .format(\"cosmos.oltp\")\n",
							"       .option(\"spark.synapse.linkedService\", \"CosmosDBLink\")\n",
							"       .option(\"spark.cosmos.container\", \"IoTDeviceInfo\")\n",
							"       .option(\"spark.cosmos.write.strategy\", \"ItemOverwrite\")  # upsert\n",
							"       .mode(\"append\")\n",
							"       .save()\n",
							")\n",
							"\n",
							"print(\"✅ IoTDeviceInfo ingested successfully.\")\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create Spark Tables Pointing to the Azure Cosmos DB Analytical Store Collections using Azure Synapse Link\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"create database if not exists CosmosDBIoTDemoDB;\n",
							"\n",
							"create table if not exists CosmosDBIoTDemoDB.IoTSignals\n",
							"using cosmos.olap\n",
							"options(spark.synapse.linkedService 'CosmosDBLink',\n",
							"        spark.cosmos.container 'IoTSignals');\n",
							"\n",
							"create table if not exists CosmosDBIoTDemoDB.IoTDeviceInfo\n",
							"using cosmos.olap\n",
							"options(spark.synapse.linkedService 'CosmosDBLink',\n",
							"        spark.cosmos.container 'IoTDeviceInfo')\n",
							""
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Perform Joins Across collections, Apply Filters and Aggregations using Spark SQL"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_RPM_details = spark.sql(\"select a.deviceid \\\n",
							"                                 , b.devicetype \\\n",
							"                                 , cast(b.location as string) as location\\\n",
							"                                 , cast(b.latitude as float) as latitude\\\n",
							"                                 , cast(b.longitude as float) as  longitude\\\n",
							"                                 , a.measuretype \\\n",
							"                                 , a.unitSymbol \\\n",
							"                                 , cast(sum(measureValue) as float) as measureValueSum \\\n",
							"                                 , count(*) as count \\\n",
							"                            from CosmosDBIoTDemoDB.IoTSignals a \\\n",
							"                            left join CosmosDBIoTDemoDB.IoTDeviceInfo b \\\n",
							"                            on a.deviceid = b.deviceid \\\n",
							"                            where a.unitSymbol = 'RPM' \\\n",
							"                            group by a.deviceid, b.devicetype, b.location, b.latitude, b.longitude, a.measuretype, a.unitSymbol\")\n",
							"\n",
							"display(df_RPM_details)"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Visualizations using Plotly and DisplayHTML()\n",
							"Heatmap of IoT signals across diffrent locations\n",
							"\n",
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/HeatMaPlot.png\" alt=\"Chart\" width=\"75%\"/>\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from plotly.offline import plot\n",
							"import plotly.express as px\n",
							"\n",
							"df_RPM_details_pd = df_RPM_details.toPandas()\n",
							"fig = px.scatter_mapbox(df_RPM_details_pd, \n",
							"                        lat='latitude', \n",
							"                        lon='longitude', \n",
							"                        size = 'measureValueSum',\n",
							"                        color = 'measureValueSum',\n",
							"                        hover_name = 'location',\n",
							"                        hover_data = ['measureValueSum','location'],\n",
							"                        size_max = 30,\n",
							"                        color_continuous_scale = px.colors.carto.Temps,\n",
							"                        zoom=3,\n",
							"                        height=600,\n",
							"                        width =900)\n",
							"\n",
							"fig.update_layout(mapbox_style='open-street-map')\n",
							"fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
							"\n",
							"p = plot(fig,output_type='div')\n",
							"displayHTML(p)"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Load the Data in Cosmos DB Analytical Store Collection into a Dataframe using Synapse Link"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"df_IoTSignals = spark.read\\\n",
							"                    .format(\"cosmos.olap\")\\\n",
							"                    .option(\"spark.synapse.linkedService\", \"CosmosDBLink\")\\\n",
							"                    .option(\"spark.cosmos.container\", \"IoTSignals\")\\\n",
							"                    .load()"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Data Exploration using Pyplot\n",
							"\n",
							"\n",
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/DataExPlot.png\" alt=\"Chart\" width=\"75%\"/>"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"df_IoTSignals_pd = df_IoTSignals.toPandas().dropna()\n",
							"df_IoTSignals_pd['measureValue'] = df_IoTSignals_pd['measureValue'].astype(int)\n",
							"\n",
							"df_MW = df_IoTSignals_pd[(df_IoTSignals_pd['deviceId'] == 'dev-1') & (df_IoTSignals_pd['unitSymbol'] == 'MW')]\n",
							"df_MW.plot(x='dateTime', y='measureValue', color='green', figsize=(20,5), label = 'Output MW')\n",
							"plt.title('MW TimeSeries')\n",
							"plt.show()\n",
							"\n",
							"df_RPM = df_IoTSignals_pd[(df_IoTSignals_pd['deviceId'] == 'dev-1') & (df_IoTSignals_pd['unitSymbol'] == 'RPM')]\n",
							"df_RPM.plot(x='dateTime', y='measureValue', color='black', figsize=(20,5), label = 'Output RPM')\n",
							"plt.title('RPM TimeSeries')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Perform Anomaly Detection using Microsoft Machine Learning for Spark (MMLSpark)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from pyspark.sql.functions import col\n",
							"from synapse.ml.services.anomaly import SimpleDetectAnomalies\n",
							"\n",
							"# 1) Configure the anomaly detector (uses synapse.ml.services, not mmlspark)\n",
							"anomaly_detector = (\n",
							"    SimpleDetectAnomalies()\n",
							"      .setSubscriptionKey(Cognitive_Services_Key)\n",
							"      .setLocation(\"southcentralus\")  # adjust if needed\n",
							"      .setOutputCol(\"anomalies\")\n",
							"      .setGroupbyCol(\"grouping\")\n",
							"      .setSensitivity(95)\n",
							"      .setGranularity(\"secondly\")\n",
							")\n",
							"\n",
							"# 2) Prepare your signal DataFrame\n",
							"signals_df = (\n",
							"    df_IoTSignals\n",
							"      .where(col(\"unitSymbol\") == \"RPM\")\n",
							"      .withColumnRenamed(\"dateTime\", \"timestamp\")\n",
							"      .withColumn(\"value\", col(\"measureValue\").cast(\"double\"))\n",
							"      .withColumn(\"grouping\", col(\"deviceId\"))\n",
							")\n",
							"\n",
							"# 3) Run the detector\n",
							"df_anomaly = anomaly_detector.transform(signals_df)\n",
							"\n",
							"# 4) Register for SQL queries\n",
							"df_anomaly.createOrReplaceTempView(\"df_anomaly\")\n",
							"\n",
							"print(\"▶️ Anomaly detection complete; temp view 'df_anomaly' is ready.\")\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# from pyspark.sql.functions import col\n",
							"# from pyspark.sql.types import *\n",
							"# from mmlspark.cognitive import SimpleDetectAnomalies\n",
							"# from mmlspark.core.spark import FluentAPI\n",
							"\n",
							"# anomaly_detector = (SimpleDetectAnomalies()\n",
							"#                             .setSubscriptionKey(Cognitive_Services_Key)\n",
							"#                             .setLocation('southcentralus') ##Optional, Update If Required\n",
							"#                             .setOutputCol(\"anomalies\")\n",
							"#                             .setGroupbyCol(\"grouping\")\n",
							"#                             .setSensitivity(95)\n",
							"#                             .setGranularity(\"secondly\"))\n",
							"\n",
							"# df_anomaly = (df_IoTSignals\n",
							"#                     .where(col(\"unitSymbol\") == 'RPM')\n",
							"#                     .withColumnRenamed(\"dateTime\", \"timestamp\")\n",
							"#                     .withColumn(\"value\", col(\"measureValue\").cast(\"double\"))\n",
							"#                     .withColumn(\"grouping\", col(\"deviceId\"))\n",
							"#                     .mlTransform(anomaly_detector))\n",
							"\n",
							"# df_anomaly.createOrReplaceTempView('df_anomaly')"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_anomaly)"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Format the Dataframe for Visualization\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_anomaly_single_device = spark.sql(\"select timestamp \\\n",
							"                                            , measureValue \\\n",
							"                                            , anomalies.expectedValue \\\n",
							"                                            , anomalies.expectedValue + anomalies.upperMargin as expectedUpperValue \\\n",
							"                                            , anomalies.expectedValue - anomalies.lowerMargin as expectedLowerValue \\\n",
							"                                            , case when anomalies.isAnomaly=true then 1 else 0 end as isAnomaly \\\n",
							"                                        from df_anomaly \\\n",
							"                                        where deviceid = 'dev-1' \\\n",
							"                                        order by timestamp \\\n",
							"                                        limit 400\")\n",
							"\n",
							"display(df_anomaly_single_device)"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Visualize the Anomalies using Plotly\n",
							"Plot Expected value, Upper Value, Lower Value and Actual Value along with Anomaly flag\n",
							"\n",
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/AnmaliesPlot.png\" alt=\"Chart\" width=\"75%\"/>\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import warnings\n",
							"warnings.filterwarnings(\n",
							"    \"ignore\",\n",
							"    category=DeprecationWarning,\n",
							"    module=\"pyspark.sql.pandas.utils\"\n",
							")\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"from pyspark.sql.functions import col\n",
							"\n",
							"# Assuming df_anomaly_single_device is already defined\n",
							"adf = df_anomaly_single_device.toPandas()\n",
							"adf_subset = adf[adf['isAnomaly'] == 1]\n",
							"\n",
							"plt.figure(figsize=(23, 8))\n",
							"plt.plot(\n",
							"    adf['timestamp'],\n",
							"    adf['expectedUpperValue'],\n",
							"    color='darkred',\n",
							"    linestyle='solid',\n",
							"    linewidth=0.25\n",
							")\n",
							"plt.plot(\n",
							"    adf['timestamp'],\n",
							"    adf['expectedValue'],\n",
							"    color='darkgreen',\n",
							"    linestyle='solid',\n",
							"    linewidth=2\n",
							")\n",
							"# Removed the 'b' format string to avoid redundant color warning\n",
							"plt.plot(\n",
							"    adf['timestamp'],\n",
							"    adf['measureValue'],\n",
							"    color='royalblue',\n",
							"    linestyle='dotted',\n",
							"    linewidth=2\n",
							")\n",
							"plt.plot(\n",
							"    adf['timestamp'],\n",
							"    adf['expectedLowerValue'],\n",
							"    color='black',\n",
							"    linestyle='solid',\n",
							"    linewidth=0.25\n",
							")\n",
							"# Use shorthand 'ro' (red circle) without extra color kw\n",
							"plt.plot(\n",
							"    adf_subset['timestamp'],\n",
							"    adf_subset['measureValue'],\n",
							"    'ro'\n",
							")\n",
							"\n",
							"plt.legend([\n",
							"    'RPM-UpperMargin',\n",
							"    'RPM-ExpectedValue',\n",
							"    'RPM-ActualValue',\n",
							"    'RPM-LowerMargin',\n",
							"    'RPM-Anomaly'\n",
							"])\n",
							"plt.title('RPM Anomalies with Expected, Actual, Upper and Lower Values')\n",
							"plt.show()\n",
							""
						],
						"outputs": [],
						"execution_count": 35
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ws1sparkpool1')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 40,
					"minNodeCount": 3
				},
				"nodeCount": 5,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sfrxvi5p4dlnq3hepocws1p1')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}